import datetime
import json
import os
import time

from . import models
from github import Github
from github.GithubException import UnknownObjectException

def mine_repo(args):
    extractor = models.GitExtractor()
    git_logs = dict()
    if(args.online):
        try:
            git_logs = extractor.get_online_logs(args.online)
        except FileNotFoundError as ex:
            print(ex)
            git_logs = dict()
    elif(args.local):
        try:
            git_logs = extractor.get_local_logs(args.local)
        except FileNotFoundError as ex:
            print(ex)
            git_logs = dict()

    return git_logs


def manage_cves(args):
    manager = models.CveManager(args.password)
    database = models.DatabaseManager(None, None)
    if args.download:
        manager.setup_cves()

    if args.database:
        manager.setup_database()

    if args.update:
        database.calculate_stats_mysql()

    if args.upload_local:
        language = args.language
        if args.language == 'unknown':
            language = input('Enter project language: ')

        if os.path.isdir(args.upload_local):
            os.chdir(args.upload_local)
            files = os.listdir()
            for file in files:
                if not file.endswith('.json'):
                    continue
                database = models.DatabaseManager(manager.read_json(file, language), None)
                database.insert_mysql()


def get_stats(args, num=None):
    stats = dict()
    if args.online:
        project = get_poject_name(args.online)
    elif args.local:
        project = get_poject_name(args.local)
    # get_stats mines the classified commits using the classes provided
    classifier =  models.CommitClassifier(args.password)
    cve_commits = classifier.get_cve_commits(mine_repo(args))
    # stats are calculated for every cve commit
    for cve in cve_commits:
        if cve_commits[cve].foundCommit != cve_commits[cve].fixCommit:
            stats[cve] = models.CveStatistics(
                project = project,
                active_contributors= cve_commits[cve].contributors,
                active_contributor_count = len(cve_commits[cve].contributors),
                commit_count = cve_commits[cve].between,
                severity = cve_commits[cve].severity,
                base_score = cve_commits[cve].base_score,
                insertions = cve_commits[cve].foundCommit['insertions'] + cve_commits[cve].fixCommit['insertions'],
                deletions = cve_commits[cve].foundCommit['deletions'] + cve_commits[cve].fixCommit['deletions'],
                files_changed = cve_commits[cve].foundCommit['files_changed'] + cve_commits[cve].fixCommit['files_changed'],
                found_date = cve_commits[cve].foundCommit['commit_date'],
                fixed_date = cve_commits[cve].fixCommit['commit_date'],
                cwe = cve_commits[cve].cwe,
                language=args.language,
            )
        else:
            stats[cve] = models.CveStatistics(
                project = project,
                active_contributors= cve_commits[cve].contributors,
                active_contributor_count = len(cve_commits[cve].contributors),
                commit_count = cve_commits[cve].between,
                severity = cve_commits[cve].severity,
                base_score = cve_commits[cve].base_score,
                insertions = cve_commits[cve].foundCommit['insertions'],
                deletions = cve_commits[cve].foundCommit['deletions'],
                files_changed = cve_commits[cve].foundCommit['files_changed'],
                found_date = cve_commits[cve].foundCommit['commit_date'],
                fixed_date = cve_commits[cve].fixCommit['commit_date'],
                cwe = cve_commits[cve].cwe,
                language=args.language,
            )
        stats[cve].time_elapsed = models.dateParser(cve_commits[cve].fixCommit['commit_date']) - models.dateParser(cve_commits[cve].foundCommit['commit_date'])
        if models.dateParser(cve_commits[cve].fixCommit['commit_date']) > models.dateParser(str(cve_commits[cve].published_date)):
            stats[cve].from_published = models.dateParser(cve_commits[cve].fixCommit['commit_date']) - models.dateParser(str(cve_commits[cve].published_date))
        else:
            stats[cve].from_published = datetime.timedelta()

    if len(stats) != 0:
        print(len(stats), 'cve-s have been found.')
        if args.store_db:
            db_manager = models.DatabaseManager(stats, args.apikey)
            #db_manager.insert_db()
            db_manager.insert_mysql()
        global_stats = models.GlobalStatistics(stats)
        global_results = global_stats.calculate_correlation()
        averages = global_stats.calculate_averages()
        stats['correlation_coefficient'] = global_results['corr']
        stats['correlation_from_published_dates'] = global_results['corr_published']
        stats['correlation_files_changed'] = global_results['corr_files']
        stats['correlation_lines_changed'] = global_results['corr_lines']
        stats['correlation_commit_count'] = global_results['corr_commits']
        stats['average_fix_time'] = averages['avg_time']
        stats['average_time_from_published_dates'] = averages['avg_published']
        stats['average_files_changed'] = averages['avg_files_changed']
        stats['average_lines_changed'] = averages['avg_lines_changed']
        print('creating stats.json')
        if not num:
            with open('stats.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=4, cls=models.StatEncoder)
        else:
            with open('stats' + str(num) +'.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=4, cls=models.StatEncoder)
    else:
        print('No cve-s have been found. No stats.json will be created.')

def mine_json_from_dir(args):
    if os.path.isdir(args.json):
        os.chdir(args.json)
        # list all files in the given directory
        files = os.listdir()
        # try to mine every json file
        for file in files:
            if file.endswith('.json') and file!='settings.json':
                parse_ghtorrent_json(
                    models.DummyArgs(
                        password=args.password,
                        apikey=args.apikey,
                        store_db=args.store_db,
                        json=args.json,
                        language=file.split('.')[0]
                    ), file
                )
        os.chdir(os.environ['ROOTDIR'])
    else:
        print('The specified directory does not exist')


def parse_ghtorrent_json(args, file):
    # num is a temporary solution to make multi repo stats possible without a db
    num = 0
    try:
        with open(file, 'r', encoding='utf-8') as f:
            repo_list = json.load(f)
    except FileNotFoundError as ex:
        print('The json file was not found')
        return

    os.chdir(os.environ['ROOTDIR'])
    # create folders for every json
    dir_name = file.split('.')[0]
    if os.path.isdir(dir_name):
        os.chdir(dir_name)
    else:
        os.mkdir(dir_name)
        os.chdir(dir_name)
    
    # mine each repo specified in the json file
    for repo in repo_list:
        args.online = parse_url(repo['url'])
        # here a each repo gets checked since private repos can't automatically be downloaded
        try:
            if not Github(os.environ['GITHUBTOKEN']).get_repo(args.online.split('/')[-2] + '/' + args.online.split('/')[-1]).private:
                # this is a safety as to not overstep the rate limit
                time.sleep(1)
                if 'language' in repo:
                    args.language = repo['language']
                get_stats(args, num)
                num = num + 1
        except Exception as ex:
            print(ex)            
    os.chdir(os.environ['ROOTDIR'])
    os.chdir(args.json)

# function separated for visibilty
def parse_url(repo_url):
    parts = repo_url.split('/')
    return parts[0] + '//' + parts[2].split('.')[1] + '.' + parts[2].split('.')[2] + '/' + parts[4] + '/' + parts[5]

# simple function to get a projects name
def get_poject_name(url = ''):
    items = url.split('/')
    items.reverse()

    for item in items:
        if item:
            return item
            