import datetime
import json
import iso8601
import pytz
import os

from . import models

def mine_repo(args):
    extractor = models.GitExtractor()
    git_logs = dict()
    if(args.online):
        try:
            git_logs = extractor.get_online_logs(args.online)
        except FileNotFoundError as ex:
            print(ex)
            git_logs = dict()
    elif(args.local):
        try:
            git_logs = extractor.get_local_logs(args.local)
        except FileNotFoundError as ex:
            print(ex)
            git_logs = dict()

    return git_logs


def manage_cves(args):
    manager = models.CveManager()
    if args.download:
        manager.setup_cves()

    if args.database:
        manager.setup_database()

def get_stats(args, num=None, insert_db=False):
    stats = dict()
    # get_stats is mines are classifies commits using the classes provided
    if args.online:
        project = (args.online).split('/')[-1]
    elif args.local:
        project = (args.local).split('/')[-1]
    classifier =  models.CommitClassifier(args.password)
    cve_commits = classifier.get_cve_commits(mine_repo(args))
    # stats are calculated for every cve commit
    for cve in cve_commits:
        stats[cve] = models.CveStatistics()
        stats[cve].project = project
        stats[cve].active_contributors = cve_commits[cve].contributors
        stats[cve].active_contributor_count = len(cve_commits[cve].contributors)
        stats[cve].commit_count = cve_commits[cve].between
        stats[cve].time_elapsed = dateParser(cve_commits[cve].fixCommit['commit_date']) - dateParser(cve_commits[cve].foundCommit['commit_date'])
        stats[cve].severity = cve_commits[cve].severity
        stats[cve].base_score = cve_commits[cve].base_score
        if dateParser(cve_commits[cve].fixCommit['commit_date']) > dateParser(str(cve_commits[cve].published_date)):
            stats[cve].published_date = dateParser(cve_commits[cve].fixCommit['commit_date']) - dateParser(str(cve_commits[cve].published_date))
        else:
            stats[cve].published_date = datetime.timedelta()

    if len(stats) != 0:
        if insert_db:
            db_manager = models.DatabaseManager(stats, args.apikey)
            db_manager.insert_db()
        global_stats = models.GlobalStatistics(stats)
        global_stats.get_stats()
        global_results = global_stats.calculate_correlation()
        averages = global_stats.calculate_averages()
        stats['correlation_coefficient'] = global_results['corr']
        stats['correlation_from_published_dates'] = global_results['corr_published']
        stats['average_fix_time'] = averages['avg_time']
        stats['average_time_from_published_dates'] = averages['avg_published']
        print('creating stats.json')
        if not num:
            with open('stats.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=4, cls=models.StatEncoder)
        else:
            with open('stats' + str(num) +'.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=4, cls=models.StatEncoder)
    else:
        print('No cve-s have been found. No stats.json will be created.')

# since gitlogparser is a separate tool, the reparsing of dates is needed
def dateParser(date_string):
    format_string = '%Y-%m-%j %H:%M:%S%z'
    date_copy = date_string
    try:
        time = iso8601.parse_date(date_string)
        return time
    except Exception as ex:
        print(ex)
        return None
# reads the json file given in the arguments, it is assumed to be in the dir where the user gives the command
def parse_ghtorrent_json(args):
    class DummyArgs(object):
        def __init__(self, url, password, apikey):
            self.online = url
            self.password = password
            self.apikey = apikey
    # num is a temporary solution to make multi repo stats possbile with a db
    num = 0
    try:
        with open(args.json, 'r', encoding='utf-8') as f:
            repo_list = json.load(f)
    except FileNotFoundError as ex:
        print('The json file was not found')

    for repo in repo_list:
        get_stats(DummyArgs(parse_url(repo['url']), args.password, args.apikey), num)
        num = num + 1

# function separated for visibilty
def parse_url(repo_url):
    parts = repo_url.split('/')
    return parts[0] + '//' + parts[2].split('.')[1] + '.' + parts[2].split('.')[2] + '/' + parts[4] + '/' + parts[5]