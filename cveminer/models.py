import subprocess
import os
import json
import re
import numpy
import datetime
import airtable
import iso8601
import pytz
import pymysql
import progressbar
import time

import gitlogparser.parser as gitParser
import cvemanager.cve_manager as cveManager
import cvemanager.cve_dbms as database

from github import Github

class GitExtractor(object):
    def __init__(self):
        self.logs = list()
        # this is where the repositories will be cloned
        self.repo_path = './repos'
        self.baseDir = os.getcwd()

    def get_online_logs(self, target_url):
        self.download_repo(target_url)
        # prep the directory, since download repo already enters the repo path, it doesn't need to be added
        directory = './' + target_url.split('/')[-1]
        if os.path.isdir(directory):
            return self.get_local_logs(directory)
        else:
            os.chdir(self.baseDir)
            raise FileNotFoundError('The specified directory couldn\'t be downloaded.')

    def get_local_logs(self, target_path):                
        if os.path.isdir(target_path):
            try:
            # parse the git logs
                print('Starting git log parser.')
                #gitParser.get_log(DummyArgs(dir = target_path, ght='foo'))

            # read the git logs
                if os.path.isfile('logdata_new.json'):
                    with open('logdata_new.json', 'r', encoding='utf-8') as f:
                        self.logs = json.load(f)
            # remove the temporary json file
                    print('Removing temporary json')
                    #os.remove('logdata_new.json')
                os.chdir(self.baseDir)
                
                os.chdir(target_path)
                # this is need to later get the necessary merge diff logs
                # the repo url will be used to establish a connection to the repo
                url = subprocess.getoutput('git config --get remote.origin.url').split('.')[1]
                url = list(filter(None, url.split('/')))
                os.environ['REPO_URL'] = url[-2] + '/' + url[-1]
                os.chdir(self.baseDir)

                return self.logs
            except Exception as ex:
                os.chdir(self.baseDir)
                print(ex)

        else:
            raise FileNotFoundError('The specified directory doesn\'t exist!')

    def download_repo(self, target_url):
        # try to open the repo path, if it fails, create a new directory
        try:
            os.chdir(self.repo_path)
        except FileNotFoundError:
            os.mkdir(self.repo_path)
            os.chdir(self.repo_path)

        # clone the requested url
        subprocess.run(['git', 'clone', target_url])


class CveManager(object):
    def __init__(self, password = None):
        self.cve_dir = None
        self.csv_dir = None
        self.user = None
        self.host = None
        self.name = None
        self.owner = None
        self.password = password

    def read_settings(self):
        cur_dir = os.getcwd()
        os.chdir(os.environ['ROOTDIR'])
        with open('settings.json', 'r') as f:
            settings = json.load(f)
            self.cve_dir = settings["paths"]["cve_path"]
            self.csv_dir = settings["paths"]["csv_path"]
            self.user = settings['database']['user']
            self.host = settings['database']['host']
            self.name = settings['database']['name']
            self.owner = settings['database']['owner']
            if self.password is None:
                self.password = input('Enter password for %s: ' % settings['database']['user'])
        os.chdir(cur_dir)

    def setup_cves(self):
        self.read_settings()
        cveManager.download_cves(self.cve_dir, False)
        cveManager.process_cves(self.cve_dir, self.csv_dir, True)

    def setup_database(self):
        self.read_settings()
        database.create_database(self.user, self.password, self.host, self.name, self.owner)
        database.create_tables(self.user, self.password, self.host, self.name)
        database.import_database(self.csv_dir, self.user, self.password, self.host, self.name)

    def get_cve(self, cve_id):
        if self.user is None or self.password is None:
            self.read_settings()
        selected_cve = database.execute_query(self.user, self.password, self.host, self.name, cve_id, date=0)
        if not selected_cve:
            return None
        return selected_cve

    def get_cwe(self, cve_id):
        if self.user is None or self.password is None:
            self.read_settings()
        selected_cwe = database.query_for_cwe(self.user, self.password, self.host, self.name, cve_id)
        if not selected_cwe:
            return None
        return selected_cwe

# class to store data about found cve
class CveData(object):
    def __init__(self, foundCommit = None, fixCommit = None, between = 1, contributors = list(),
     severity = None, base_score=None, published_date=None, files_changed = 0, insertions = 0, deletions = 0, cwe = None):
        self.foundCommit = foundCommit
        self.fixCommit = fixCommit
        self.between = between
        self.contributors = contributors
        self.severity = severity
        self.base_score = base_score
        self.published_date = published_date
        self.insertions = insertions
        self.deletions = deletions
        self.files_changed = files_changed
        self.cwe = cwe

    def __str__(self):
        return f'start commit: {self.foundCommit}, fixing commit: {self.fixCommit}, between them {self.between} commit(s) have been made by {len(self.contributors)} contributors, {self.insertions} lines have been added, {self.deletions} have been deleted and {self.files_changed} files have been changed.'

#class to calssify commits as cve
class CommitClassifier(object):
    def __init__(self, password):
        self.cve_id_pattern = re.compile(r'(CVE-(1999|2\d{3})-(0\d{2}[0-9]|[1-9]\d{3,}))', re.IGNORECASE)
        self.password = password
    # function that gets every commit that looks like a cve and returns it
    def get_cve_commits(self, commits):
        def setup_cve_data(cve):
            cve_commits[cve].contributors.append(commit['author'])
            cve_commits[cve].insertions = cve_commits[cve].foundCommit['insertions']
            cve_commits[cve].deletions = cve_commits[cve].foundCommit['deletions']
            cve_commits[cve].files_changed = cve_commits[cve].foundCommit['files_changed']

        cve_commits = dict()
        cve_manager = CveManager(self.password)
        commits.sort(key=self.commit_sort)

        # setup the API connection to the repo
        repo = Github(os.environ['GITHUBTOKEN']).get_repo(os.environ['REPO_URL'])
        # look through mined commits and collect every cve
        print("Parsing commits")
        for commit in progressbar.progressbar(commits):
            if commit['message'] and self.cve_id_pattern.search(commit['message']):
                cve_group = re.findall(self.cve_id_pattern, commit['message'])
                # get data stored about the cve in the db
                for cve in cve_group:
                    cve = cve[0].upper()
                    cve_db_data = cve_manager.get_cve(cve)
                    if not cve_db_data:
                        continue
                    elif commit['isMerge']:
                        # merge commit stats are not parsed, so it is necessary to get them here
                        try:
                            response = repo.get_commit(sha=commit['commit_hash'])
                            commit['insertions'] = response.stats.additions
                            commit['deletions'] = response.stats.deletions
                            commit['files_changed'] = len(response.files)
                        except Exception as ex:
                            print(ex)
                            print('Merge commit unavailable')
                        # a sleep is needed to not overwhelm the API
                        time.sleep(0.73)
                        
                    cwe_group = cve_manager.get_cwe(cve)
                    # if a cve only appears once it is assumed to be a fix
                    # if a cve is not in the database it is considered to be invalid and therefore not considered
                    if cve not in cve_commits.keys():
                        cve_commits[cve] = CveData(commit, commit, contributors=list())
                        cve_commits[cve].base_score = cve_db_data[0][5]
                        if cwe_group and cwe_group[0][1]:
                            cve_commits[cve].cwe = cwe_group[0][1]
                        if cve_db_data[0][6]:
                            cve_commits[cve].severity = cve_db_data[0][6].rstrip()
                        elif cve_commits[cve].base_score:
                            if 0 <= cve_commits[cve].base_score < 4:
                                cve_commits[cve].severity = "LOW"
                            elif 4 <= cve_commits[cve].base_score < 7:
                                cve_commits[cve].severity = "MEDIUM"
                            else:
                                cve_commits[cve].severity = "HIGH"
                        else:
                            cve_commits[cve].severity = None
                        cve_commits[cve].published_date = cve_db_data[0][8]
                    else:
                        cve_commits[cve].fixCommit = commit

        # a for cycle is needed, since I have to recheck every inbetween commit
        # for their data, since at this point I know when the cve has been fixed
        for cve in cve_commits:
            started = False
            if cve_commits[cve].foundCommit == cve_commits[cve].fixCommit:
                setup_cve_data(cve)
                continue
            else:
                for commit in commits:
                    if started:
                        if commit['author'] not in cve_commits[cve].contributors:
                            cve_commits[cve].contributors.append(commit['author'])

                        cve_commits[cve].between = cve_commits[cve].between + 1
                        cve_commits[cve].insertions = cve_commits[cve].insertions + commit['insertions']
                        cve_commits[cve].deletions = cve_commits[cve].deletions + commit['deletions']
                        cve_commits[cve].files_changed = cve_commits[cve].files_changed + commit['files_changed']


                        if cve_commits[cve].fixCommit == commit:
                            break

                    elif cve_commits[cve].foundCommit == commit:
                        started = True
                        setup_cve_data(cve)

        return cve_commits

    def commit_sort(self, e):
        return dateParser(e['commit_date'])


# class to store statisctics the have been calculated from CveData objects
class CveStatistics(object):
    def __init__(self, active_contributors = list(), active_contributor_count = 0, commit_count = 0, time_elapsed = None, severity=None, base_score=None, from_published=None, project=None,
        insertions = 0, deletions = 0, files_changed = 0, found_date = None, fixed_date = None, cwe = None, language = None):
        self.active_contributors = active_contributors
        self.active_contributor_count = active_contributor_count
        self.commit_count = commit_count
        self.time_elapsed = time_elapsed
        self.severity = severity
        self.base_score = base_score
        self.from_published = from_published
        self.project = project
        self.insertions = insertions
        self.deletions = deletions
        self.files_changed = files_changed
        self.total_lines_changed = insertions + deletions
        self.found_date = found_date
        self.fixed_date = fixed_date
        self.cwe = cwe
        self.language = language
    def __str__(self):
        return f'{self.active_contributors}, {self.cwe}, {self.active_contributor_count}, {self.commit_count}, {self.time_elapsed}, {self.severity}, {self.base_score}, {self.published_date}, {self.project}'

    def to_json(self):
        return{
            'project': self.project,
            'active_contributors': self.active_contributors,
            'active_contributor_count': self.active_contributor_count,
            'commit_count': self.commit_count,
            'insertions': self.insertions,
            'deletions': self.deletions,
            'total_lines_changed': self.total_lines_changed,
            'files_changed': self.files_changed,
            'found_date': self.found_date,
            'fixed_date': self.fixed_date,
            'time_elapsed': str(self.time_elapsed),
            'cwe_group': self.cwe,
            'severity': self.severity,
            'base_score': self.base_score,
            'from_published': str(self.from_published),
            'language': self.language,
        }

class GlobalStatistics(object):
    def __init__(self, stats=None):
        self.stats = stats
        self.base_scores = list()
        self.contributors = list()
        self.fix_times = list()
        self.fix_times_published = list()
        self.total_lines_changed = list()
        self.files_changed = list()
        self.commit_counts = list()
        self.get_stats()

    def get_stats(self):
        for cve in self.stats.keys():
            if self.stats[cve].base_score:
                self.base_scores.append(self.stats[cve].base_score)
                self.contributors.append(self.stats[cve].active_contributor_count)
                self.fix_times.append(self.stats[cve].time_elapsed.total_seconds())
                self.fix_times_published.append(self.stats[cve].from_published.total_seconds())
                self.total_lines_changed.append(self.stats[cve].total_lines_changed)
                self.files_changed.append(self.stats[cve].files_changed)
                self.commit_counts.append(self.stats[cve].commit_count)

    def calculate_correlation(self):
        if not self.stats:
            return None

        result = dict()

        if len(self.stats) < 2:
            result['corr'] = None
            result['corr_published'] = None
            result['corr_files'] = None
            result['corr_lines'] = None
            result['corr_commits'] = None
            return result

        corr = numpy.corrcoef(self.base_scores, self.fix_times)
        corr_published = numpy.corrcoef(self.base_scores, self.fix_times_published)
        corr_files = numpy.corrcoef(self.base_scores, self.files_changed)
        corr_lines = numpy.corrcoef(self.base_scores, self.total_lines_changed)
        corr_commits = numpy.corrcoef(self.base_scores, self.commit_counts)

        result['corr'] = corr[0, 1]
        result['corr_published'] = corr_published[0, 1]
        result['corr_files'] = corr_files[0, 1]
        result['corr_lines'] = corr_lines[0, 1]
        result['corr_commits'] = corr_commits[0, 1]
        return result

    def calculate_averages(self):
        if not self.stats:
            return None

        result = dict()

        result['avg_time'] = str(datetime.timedelta(seconds=numpy.mean(self.fix_times)))
        result['avg_published'] = str(datetime.timedelta(seconds=numpy.mean(self.fix_times_published)))
        result['avg_lines_changed'] = str(numpy.mean(self.total_lines_changed))
        result['avg_files_changed'] = str(numpy.mean(self.files_changed))

        return result


class StatEncoder(json.JSONEncoder):
    def default(self, stats):
        # creates a list out of the mined commits
        if isinstance(stats, CveStatistics):
            return stats.to_json()
        return super(StatEncoder, self).default(stats)

class DatabaseManager(object):
    def __init__(self, stats, apikey):
        self.stats = stats
        self.apikey = apikey

    def insert_db(self):
        if self.apikey is None:
            self.apikey = input('Enter api key: ')

        print("Uploading to Airtable")

        new_entries = False
        cve_table = airtable.Airtable('appX3T3GA3Tim89PA', 'cve_data', api_key=self.apikey)
        for cve in self.stats.keys():
            existing_records = cve_table.search('cve_key', self.stats[cve].project + '-' + cve)

            if len(existing_records) == 0:
                new_entries = True
                record = {
                    'cve_id': cve,
                    'project': self.stats[cve].project,
                    'cwe_group': self.stats[cve].cwe,
                    'active_contributor_count': self.stats[cve].active_contributor_count,
                    'commit_count': self.stats[cve].commit_count,
                    'insertions' : self.stats[cve].insertions,
                    'deletions' : self.stats[cve].deletions,
                    'total_lines_changed' : self.stats[cve].total_lines_changed,
                    'files_changed' : self.stats[cve].files_changed,
                    'time_elapsed': str(self.stats[cve].time_elapsed),
                    'found_date': self.stats[cve].found_date,
                    'fixed_date': self.stats[cve].fixed_date,
                    'severity': self.stats[cve].severity,
                    'base_score': self.stats[cve].base_score,
                    'from_published': str(self.stats[cve].from_published),
                    'language': self.stats[cve].language
                }
                cve_table.insert(record)

        if new_entries:
            self.calculate_stats(cve_table)

    # this is a temporary solution so the use of the function may not be supported in the future
    # as such it is not flexible
    def insert_mysql(self):
        host = "s1.sootsoft.hu"
        user = "cve_data"
        password = "aDjA4vaEsIjn5ZCF"
        con = pymysql.connect(
            host=host,
            user=user,
            password=password,
            cursorclass=pymysql.cursors.DictCursor
            )

        insertion_text = """INSERT INTO cve_data.test_data
        (cve_id, project, cwe_group, active_contributor_count,
         commit_count, deletions, insertions, total_lines_changed, files_changed,
          time_elapsed, found_date, fixed_date, severity, base_score,
           from_published, language)
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""

        for cve in self.stats:
            con.cursor().execute(
                insertion_text,
                (
                    cve,
                    self.stats[cve].project,
                    self.stats[cve].cwe,
                    self.stats[cve].active_contributor_count,
                    self.stats[cve].commit_count,
                    self.stats[cve].insertions,
                    self.stats[cve].deletions,
                    self.stats[cve].total_lines_changed,
                    self.stats[cve].files_changed,
                    str(self.stats[cve].time_elapsed),
                    str(self.stats[cve].found_date),
                    str(self.stats[cve].fixed_date),
                    self.stats[cve].severity,
                    self.stats[cve].base_score,
                    str(self.stats[cve].from_published),
                    self.stats[cve].language
                )
            )
        con.commit()
        con.close()

    # this is a temporary solution for uploading stats to the mysql db
    def calculate_stats_mysql(self):
        print('Updating stats')

        tm = TimeManager()

        host = "s1.sootsoft.hu"
        user = "cve_data"
        password = "aDjA4vaEsIjn5ZCF"
        con = pymysql.connect(
            host=host,
            user=user,
            password=password,
            cursorclass=pymysql.cursors.DictCursor
        )

        cursor = con.cursor()
        cursor.execute('SELECT DISTINCT language FROM cve_data.test_data')

        langs = cursor.fetchall()
        for lang in langs:
            stats = dict()
            cursor.execute('SELECT * FROM cve_data.test_data WHERE language = %s', lang['language'])
            for data in cursor.fetchall():
                cve = data['cve_id']
                stats[cve] = CveStatistics(
                    commit_count=data['commit_count'],
                    base_score=data['base_score'],
                    insertions=data['insertions'],
                    deletions=data['deletions'],
                    files_changed=data['files_changed'],
                    time_elapsed=tm.to_timedelta(data['time_elapsed']),
                    from_published=tm.to_timedelta(data['from_published'])
                )

            global_stats = GlobalStatistics(stats)
            correlations = global_stats.calculate_correlation()
            averages = global_stats.calculate_averages()

            cursor.execute('SELECT language FROM cve_data.cve_stats WHERE language = %s', lang['language'])
            if cursor.fetchall():
                cursor.execute('UPDATE cve_data.cve_stats SET correlation = %s WHERE language = %s', correlations['corr'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET corr_files = %s WHERE language = %s', correlations['corr_files'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET corr_lines = %s WHERE language = %s', correlations['corr_lines'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET avg_fix = %s WHERE language = %s', averages['avg_time'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET avg_fix_published = %s WHERE language = %s', averages['avg_published'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET avg_files = %s WHERE language = %s', averages['avg_files_changed'], lang['language'])
                cursor.execute('UPDATE cve_data.cve_stats SET avg_lines = %s WHERE language = %s', averages['avg_lines_changed'], lang['language'])
            else:
                insertion_text = 'INSERT INTO cve_data.cve_stats VALUES (%s, %s, %s)'
                cursor.execute(insertion_text, (lang['language'], 'correlation', str(correlations['corr'])))
                cursor.execute(insertion_text, (lang['language'], 'corr_files', str(correlations['corr_files'])))
                cursor.execute(insertion_text, (lang['language'], 'corr_lines', str(correlations['corr_lines'])))
                cursor.execute(insertion_text, (lang['language'], 'avg_fix', averages['avg_time']))
                cursor.execute(insertion_text, (lang['language'], 'avg_fix_published', averages['avg_published']))
                cursor.execute(insertion_text, (lang['language'], 'avg_files', averages['avg_files_changed']))
                cursor.execute(insertion_text, (lang['language'], 'avg_lines', averages['avg_lines_changed']))
        con.commit()
        con.close()

    def calculate_stats(self, table):
        stats_table = airtable.Airtable('appX3T3GA3Tim89PA', 'stats', api_key=self.apikey)

        tm = TimeManager()
        stats = dict()
        cves = table.get_all(fields=['cve_id', 'active_contributor_count', 'time_elapsed', 'base_score', 'from_published'])
        for record in cves:
            cve = record['fields']['cve_id']
            stats[cve] = CveStatistics()
            stats[cve].active_contributors = record['fields']['active_contributor_count']
            stats[cve].time_elapsed = tm.to_timedelta(record['fields']['time_elapsed'])
            stats[cve].base_score = record['fields']['base_score']
            stats[cve].from_published = tm.to_timedelta(record['fields']['from_published'])

        gs = GlobalStatistics(stats)
        cors = gs.calculate_correlation()
        avgs = gs.calculate_averages()

        stat_record = stats_table.match('type', 'correlation_coefficient')
        stats_table.update(stat_record['id'], {'value': str(cors['corr'])})
        stat_record = stats_table.match('type', 'correlation_from_published_dates')
        stats_table.update(stat_record['id'], {'value': str(cors['corr_published'])})
        stat_record = stats_table.match('type', 'average_fix_time')
        stats_table.update(stat_record['id'], {'value': str(avgs['avg_time'])})
        stat_record = stats_table.match('type', 'average_from_published_dates')
        stats_table.update(stat_record['id'], {'value': str(avgs['avg_published'])})


class TimeManager(object):
    def __init__(self):
        pass

    def to_timedelta(self, date_string):
        days = ''
        timestamp = ''
        if date_string.find('days') != -1:
            days, timestamp = date_string.split(" days, ")
        elif date_string.find('day') != -1:
            days, timestamp = date_string.split(" day, ")
        else:
            timestamp = date_string

        t = datetime.datetime.strptime(timestamp, "%H:%M:%S")
        if days:
            return datetime.timedelta(days=int(days), hours=t.hour, minutes=t.minute, seconds=t.second)

        return datetime.timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)

# since gitlogparser is a separate tool, the reparsing of dates is needed
def dateParser(date_string):
    try:
        return iso8601.parse_date(date_string)
    except Exception as ex:
        print(ex)
        sliced_date = date_string[:date_string.rfind(' ')]
        try:
            return pytz.UTC.localize(datetime.datetime.strptime(sliced_date, '%a %b %d %H:%M:%S %Y'))
        except Exception as ex:
            print(ex)
            return None
# simple class used to simulate command line arguments
class DummyArgs(object):
        def __init__(self, url='', password='', apikey='', store_db='', language='Unkown', dir='', json='', ght=''):
            self.directory = dir
            self.online = url
            self.password = password
            self.apikey = apikey
            self.store_db = store_db
            self.language = language
            self.json=json
            self.github_token = ght
            self.no_merge = True
        
        def __str__(self):
            return f"""
                    online: {self.online},
                    dir: {self.directory},
                    pw: {self.password},
                    apikey: {self.apikey},
                    store_db: {self.store_db},
                    lang: {self.language},
                    json: {self.json},
                    ght: {self.github_token}
                    """