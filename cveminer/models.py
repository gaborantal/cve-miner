import subprocess
import os
import json
import re
import numpy
import datetime
import airtable
import iso8601
import pytz
import pymysql
import progressbar
import time
import psycopg2

import gitlogparser.parser as gitParser
import cvemanager.cve_manager as cveManager
import cvemanager.cve_dbms as database

from github import Github
from psycopg2 import connect
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT

class GitExtractor(object):
    def __init__(self):
        self.logs = list()
        # this is where the repositories will be cloned
        self.repo_path = './repos'
        self.baseDir = os.getcwd()

    def get_online_logs(self, target_url):
        self.download_repo(target_url)
        # prep the directory, since download repo already enters the repo path, it doesn't need to be added
        directory = './' + target_url.split('/')[-1]
        if os.path.isdir(directory):
            return self.get_local_logs(directory)
        else:
            os.chdir(self.baseDir)
            raise FileNotFoundError('The specified directory couldn\'t be downloaded.')

    def get_local_logs(self, target_path):                
        if os.path.isdir(target_path):
            try:
            # parse the git logs
                print('Starting git log parser.')
                gitParser.get_log(DummyArgs(dir = target_path, ght='foo'))

            # read the git logs
                if os.path.isfile('logdata_new.json'):
                    with open('logdata_new.json', 'r', encoding='utf-8') as f:
                        self.logs = json.load(f)
            # remove the temporary json file
                    print('Removing temporary json')
                    os.remove('logdata_new.json')                
                os.chdir(target_path)
                # this is need to later get the necessary merge diff logs
                # the repo url will be used to establish a connection to the repo
                url = subprocess.getoutput('git config --get remote.origin.url').split('.')[1]
                url = list(filter(None, url.split('/')))
                os.environ['REPO_URL'] = url[-2] + '/' + url[-1]
                os.chdir(self.baseDir)

                return self.logs
            except Exception as ex:
                os.chdir(self.baseDir)
                print(ex)

        else:
            raise FileNotFoundError('The specified directory doesn\'t exist!')

    def download_repo(self, target_url):
        # try to open the repo path, if it fails, create a new directory
        try:
            os.chdir(self.repo_path)
        except FileNotFoundError:
            os.mkdir(self.repo_path)
            os.chdir(self.repo_path)

        # clone the requested url
        subprocess.run(['git', 'clone', target_url])


class CveManager(object):
    def __init__(self, password = None):
        self.cve_dir = None
        self.csv_dir = None
        self.user = None
        self.host = None
        self.name = None
        self.owner = None
        self.password = password

    def read_settings(self):
        cur_dir = os.getcwd()
        os.chdir(os.environ['ROOTDIR'])
        with open('settings.json', 'r') as f:
            settings = json.load(f)
            self.cve_dir = settings["paths"]["cve_path"]
            self.csv_dir = settings["paths"]["csv_path"]
            self.user = settings['database']['user']
            self.host = settings['database']['host']
            self.name = settings['database']['name']
            self.owner = settings['database']['owner']
            if self.password is None:
                self.password = input('Enter password for %s: ' % settings['database']['user'])
        os.chdir(cur_dir)

    def setup_cves(self):
        self.read_settings()
        cveManager.download_cves(self.cve_dir, False)
        cveManager.process_cves(self.cve_dir, self.csv_dir, True)

    def setup_database(self):
        self.read_settings()
        database.create_database(self.user, self.password, self.host, self.name, self.owner)
        database.create_tables(self.user, self.password, self.host, self.name)
        database.import_database(self.csv_dir, self.user, self.password, self.host, self.name)
        self.create_local_table()

    def get_cve(self, cve_id):
        if self.user is None or self.password is None:
            self.read_settings()
        selected_cve = database.execute_query(self.user, self.password, self.host, self.name, cve_id, date=0)
        if not selected_cve:
            return None
        return selected_cve

    def get_cwe(self, cve_id):
        if self.user is None or self.password is None:
            self.read_settings()
        selected_cwe = database.query_for_cwe(self.user, self.password, self.host, self.name, cve_id)
        if not selected_cwe:
            return None
        return selected_cwe

    def create_local_table(self):
        dbmanager = DatabaseManager(None, None)
        dbmanager.create_local_table(self.name, self.user, self.host, self.password)

    def copy_data(self, apikey=None):
        if self.user is None or self.password is None:
            self.read_settings()
        dbmanager = DatabaseManager(None, apikey)
        dbmanager.copy_data(self.name, self.user, self.host, self.password)

    def read_json(self, file, lang):
        stats = dict()
        if file.endswith('.json') and file != 'settings.json':
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for cve in data.keys():
                    if not str(cve).startswith('CVE'):
                        break

                    stats[cve] = CveStatistics(
                        project=data[cve]['project'],
                        active_contributor_count=data[cve]['active_contributor_count'],
                        commit_count=data[cve]['commit_count'],
                        time_elapsed=data[cve]['time_elapsed'],
                        severity=data[cve]['severity'],
                        base_score=data[cve]['base_score'],
                        from_published=data[cve]['from_published'],
                        insertions=data[cve]['insertions'],
                        deletions=data[cve]['deletions'],
                        files_changed=data[cve]['files_changed'],
                        found_date=data[cve]['found_date'],
                        fixed_date=data[cve]['fixed_date'],
                        cwe=data[cve]['cwe_group'],
                        language=lang
                    )
                return stats


# class to store data about found cve
class CveData(object):
    def __init__(self, foundCommit = None, fixCommit = None, between = 1, contributors = list(),
     severity = None, base_score=None, published_date=None, files_changed = 0, insertions = 0, deletions = 0, cwe = None):
        self.foundCommit = foundCommit
        self.fixCommit = fixCommit
        self.between = between
        self.contributors = contributors
        self.severity = severity
        self.base_score = base_score
        self.published_date = published_date
        self.insertions = insertions
        self.deletions = deletions
        self.files_changed = files_changed
        self.cwe = cwe

    def __str__(self):
        return f'start commit: {self.foundCommit}, fixing commit: {self.fixCommit}, between them {self.between} commit(s) have been made by {len(self.contributors)} contributors, {self.insertions} lines have been added, {self.deletions} have been deleted and {self.files_changed} files have been changed.'

#class to calssify commits as cve
class CommitClassifier(object):
    def __init__(self, password):
        self.cve_id_pattern = re.compile(r'(CVE-(1999|2\d{3})-(0\d{2}[0-9]|[1-9]\d{3,}))', re.IGNORECASE)
        self.password = password
    # function that gets every commit that looks like a cve and returns it
    def get_cve_commits(self, commits):
        def setup_cve_data(cve):
            cve_commits[cve].contributors.append(commit['author'])
            cve_commits[cve].insertions = cve_commits[cve].foundCommit['insertions']
            cve_commits[cve].deletions = cve_commits[cve].foundCommit['deletions']
            cve_commits[cve].files_changed = cve_commits[cve].foundCommit['files_changed']

        cve_commits = dict()
        cve_manager = CveManager(self.password)
        commits.sort(key=self.commit_sort)

        # setup the API connection to the repo
        repo = Github(os.environ['GITHUBTOKEN']).get_repo(os.environ['REPO_URL'])
        # look through mined commits and collect every cve
        print("Parsing commits")
        for commit in progressbar.progressbar(commits):
            if commit['message'] and self.cve_id_pattern.search(commit['message']):
                cve_group = re.findall(self.cve_id_pattern, commit['message'])
                # get data stored about the cve in the db
                for cve in cve_group:
                    cve = cve[0].upper()
                    cve_db_data = cve_manager.get_cve(cve)
                    if not cve_db_data:
                        continue
                    elif commit['isMerge']:
                        # merge commit stats are not parsed, so it is necessary to get them here
                        try:
                            response = repo.get_commit(sha=commit['commit_hash'])
                            commit['insertions'] = response.stats.additions
                            commit['deletions'] = response.stats.deletions
                            commit['files_changed'] = len(response.files)
                        except Exception as ex:
                            print(ex)
                            print('Merge commit unavailable')
                        # a sleep is needed to not overwhelm the API
                        time.sleep(0.73)
                        
                    cwe_group = cve_manager.get_cwe(cve)
                    # if a cve only appears once it is assumed to be a fix
                    # if a cve is not in the database it is considered to be invalid and therefore not considered
                    if cve not in cve_commits.keys():
                        cve_commits[cve] = CveData(commit, commit, contributors=list())
                        cve_commits[cve].base_score = cve_db_data[0][5]
                        if cwe_group and cwe_group[0][1]:
                            cve_commits[cve].cwe = cwe_group[0][1]
                        if cve_db_data[0][6]:
                            cve_commits[cve].severity = cve_db_data[0][6].rstrip()
                        elif cve_commits[cve].base_score:
                            if 0 <= cve_commits[cve].base_score < 4:
                                cve_commits[cve].severity = "LOW"
                            elif 4 <= cve_commits[cve].base_score < 7:
                                cve_commits[cve].severity = "MEDIUM"
                            else:
                                cve_commits[cve].severity = "HIGH"
                        else:
                            cve_commits[cve].severity = None
                        cve_commits[cve].published_date = cve_db_data[0][8]
                    else:
                        cve_commits[cve].fixCommit = commit

        # a for cycle is needed, since I have to recheck every inbetween commit
        # for their data, since at this point I know when the cve has been fixed
        for cve in cve_commits:
            started = False
            if cve_commits[cve].foundCommit == cve_commits[cve].fixCommit:
                setup_cve_data(cve)
                continue
            else:
                for commit in commits:
                    if started:
                        if commit['author'] not in cve_commits[cve].contributors:
                            cve_commits[cve].contributors.append(commit['author'])

                        cve_commits[cve].between = cve_commits[cve].between + 1
                        cve_commits[cve].insertions = cve_commits[cve].insertions + commit['insertions']
                        cve_commits[cve].deletions = cve_commits[cve].deletions + commit['deletions']
                        cve_commits[cve].files_changed = cve_commits[cve].files_changed + commit['files_changed']


                        if cve_commits[cve].fixCommit == commit:
                            break

                    elif cve_commits[cve].foundCommit == commit:
                        started = True
                        setup_cve_data(cve)

        return cve_commits

    def commit_sort(self, e):
        return dateParser(e['commit_date'])


# class to store statisctics the have been calculated from CveData objects
class CveStatistics(object):
    def __init__(self, active_contributors = list(), active_contributor_count = 0, commit_count = 0, time_elapsed = None, severity=None, base_score=None, from_published=None, project=None,
        insertions = 0, deletions = 0, files_changed = 0, found_date = None, fixed_date = None, cwe = None, language = None, foundHash = None, fixHash = None):
        self.active_contributors = active_contributors
        self.active_contributor_count = active_contributor_count
        self.commit_count = commit_count
        self.time_elapsed = time_elapsed
        self.severity = severity
        self.base_score = base_score
        self.from_published = from_published
        self.project = project
        self.insertions = insertions
        self.deletions = deletions
        self.files_changed = files_changed
        self.total_lines_changed = insertions + deletions
        self.found_date = found_date
        self.fixed_date = fixed_date
        self.cwe = cwe
        self.language = language
        self.foundHash = foundHash
        self.fixHash = fixHash
    def __str__(self):
        return f'{self.active_contributors}, {self.cwe}, {self.active_contributor_count}, {self.commit_count}, {self.time_elapsed}, {self.severity}, {self.base_score}, {self.from_published}, {self.project}'

    def to_json(self):
        return{
            'project': self.project,
            'cwe': self.cwe,
            'active_contributors': self.active_contributors,
            'active_contributor_count': self.active_contributor_count,
            'commit_count': self.commit_count,
            'insertions': self.insertions,
            'deletions': self.deletions,
            'total_lines_changed': self.total_lines_changed,
            'files_changed': self.files_changed,
            'found_date': self.found_date,
            'fixed_date': self.fixed_date,
            'time_elapsed': str(self.time_elapsed),
            'cwe_group': self.cwe,
            'severity': self.severity,
            'base_score': self.base_score,
            'from_published': str(self.from_published),
            'language': self.language,
        }

class GlobalStatistics(object):
    def __init__(self, stats=None):
        self.stats = stats
        self.base_scores = list()
        self.contributors = list()
        self.fix_times = list()
        self.fix_times_published = list()
        self.total_lines_changed = list()
        self.files_changed = list()
        self.commit_counts = list()
        self.get_stats()

    def get_stats(self):
        for cve in self.stats.keys():
            if self.stats[cve].base_score:
                self.base_scores.append(self.stats[cve].base_score)
                self.contributors.append(self.stats[cve].active_contributor_count)
                self.fix_times.append(self.stats[cve].time_elapsed.total_seconds())
                self.fix_times_published.append(self.stats[cve].from_published.total_seconds())
                self.total_lines_changed.append(self.stats[cve].total_lines_changed)
                self.files_changed.append(self.stats[cve].files_changed)
                self.commit_counts.append(self.stats[cve].commit_count)

    def calculate_correlation(self):
        if not self.stats:
            return None

        result = dict()

        if len(self.stats) < 2:
            result['corr'] = None
            result['corr_published'] = None
            result['corr_files'] = None
            result['corr_lines'] = None
            result['corr_commits'] = None
            return result

        corr = numpy.corrcoef(self.base_scores, self.fix_times)
        corr_published = numpy.corrcoef(self.base_scores, self.fix_times_published)
        corr_files = numpy.corrcoef(self.base_scores, self.files_changed)
        corr_lines = numpy.corrcoef(self.base_scores, self.total_lines_changed)
        corr_commits = numpy.corrcoef(self.base_scores, self.commit_counts)

        result['corr'] = corr[0, 1]
        result['corr_published'] = corr_published[0, 1]
        result['corr_files'] = corr_files[0, 1]
        result['corr_lines'] = corr_lines[0, 1]
        result['corr_commits'] = corr_commits[0, 1]
        return result

    def calculate_averages(self):
        if not self.stats:
            return None

        result = dict()

        result['avg_time'] = str(datetime.timedelta(seconds=numpy.mean(self.fix_times)))
        result['avg_published'] = str(datetime.timedelta(seconds=numpy.mean(self.fix_times_published)))
        result['avg_lines_changed'] = str(numpy.mean(self.total_lines_changed))
        result['avg_files_changed'] = str(numpy.mean(self.files_changed))
        result['median_time'] = str(datetime.timedelta(seconds=numpy.median(self.fix_times)))
        result['median_published'] = str(datetime.timedelta(seconds=numpy.median(self.fix_times_published)))
        result['median_lines'] = str(numpy.median(self.total_lines_changed))
        result['median_files'] = str(numpy.median(self.files_changed))

        return result

    def calculate_deviations(self):
        if not self.stats:
            return None

        result = dict()

        result['sd_time'] = str(datetime.timedelta(seconds=numpy.std(self.fix_times)))
        result['sd_published'] = str(datetime.timedelta(seconds=numpy.std(self.fix_times_published)))
        result['sd_lines'] = str(numpy.std(self.total_lines_changed))
        result['sd_files'] = str(numpy.std(self.files_changed))

        return result


class StatEncoder(json.JSONEncoder):
    def default(self, stats):
        # creates a list out of the mined commits
        if isinstance(stats, CveStatistics):
            return stats.to_json()
        return super(StatEncoder, self).default(stats)

class DatabaseManager(object):
    def __init__(self, stats, apikey):
        self.stats = stats
        self.apikey = apikey

    #returns filed if it's not null, otherwise returns value
    def nvl(self, map, key, value):
        try:
            map[key]
        except Exception:
            return value
        return map[key]

    def create_local_table(self, name, user, host, password):
        table_string = '''CREATE TABLE IF NOT EXISTS public.cve_data (
                                    cve_id VARCHAR(20) NOT NULL,
                                    project VARCHAR(250) NOT NULL,
                                    cwe_group VARCHAR(20),
                                    active_contributor_count INTEGER,
                                    commit_count INTEGER,
                                    deletions INTEGER,
                                    insertions INTEGER,
                                    total_lines_changed INTEGER,
                                    files_changed INTEGER,
                                    time_elapsed VARCHAR(150),
                                    found_date VARCHAR(250),
                                    fixed_date VARCHAR(250),
                                    severity VARCHAR(20),
                                    base_score REAL,
                                    from_published VARCHAR(150),
                                    language VARCHAR(50),
                                    PRIMARY KEY(cve_id, project)
                )'''

        with connect(dbname=name, user=user, host=host, password=password) as con:
            con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
            with con.cursor() as cur:
                try:
                    cur.execute(table_string)
                except (Exception, psycopg2.DatabaseError) as error:
                    print('Error while creating tables', error)
            con.commit()

    def copy_data(self, name, user, host, password):
        if self.apikey is None:
            self.apikey = input('Enter api key: ')

        insertion_string = 'INSERT INTO cve_data VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'

        print('Querying from Airtable')
        cve_table = airtable.Airtable('appX3T3GA3Tim89PA', 'cve_data', api_key=self.apikey)
        cves = cve_table.get_all()

        print('Uploading to postgres')
        with connect(dbname=name, user=user, host=host, password=password) as con:
            con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
            with con.cursor() as cur:
                for record in progressbar.progressbar(cves):
                    values = (record['fields']['cve_id'], record['fields']['project'], self.nvl(record['fields'], 'cwe_group', None),
                              record['fields']['active_contributor_count'], record['fields']['commit_count'],
                              record['fields']['deletions'], record['fields']['insertions'], record['fields']['total_lines_changed'],
                              record['fields']['files_changed'], record['fields']['time_elapsed'], record['fields']['found_date'],
                              record['fields']['fixed_date'], self.nvl(record['fields'], 'severity', None),
                              self.nvl(record['fields'], 'base_score', None), record['fields']['from_published'],
                              record['fields']['language'])
                    cur.execute(insertion_string, values)
            con.commit()

    def insert_db(self):
        if self.apikey is None:
            self.apikey = input('Enter api key: ')

        print("Uploading to Airtable")
        cve_table = airtable.Airtable('appX3T3GA3Tim89PA', 'cve_data', api_key=self.apikey)
        for cve in self.stats.keys():
            existing_records = cve_table.search('cve_key', self.stats[cve].project + '-' + cve)
            print(self.stats[cve].found_date)
            print(self.stats[cve].foundHash)
            print(self.stats[cve].fixHash)
            if len(existing_records) == 0:
                record = {
                    'cve_id': cve,
                    'project': self.stats[cve].project,
                    'cwe_group': self.stats[cve].cwe,
                    'active_contributor_count': self.stats[cve].active_contributor_count,
                    'commit_count': self.stats[cve].commit_count,
                    'insertions' : self.stats[cve].insertions,
                    'deletions' : self.stats[cve].deletions,
                    'total_lines_changed' : self.stats[cve].total_lines_changed,
                    'files_changed' : self.stats[cve].files_changed,
                    'time_elapsed': str(self.stats[cve].time_elapsed),
                    'found_date': self.stats[cve].found_date,
                    'fixed_date': self.stats[cve].fixed_date,
                    'severity': self.stats[cve].severity,
                    'base_score': self.stats[cve].base_score,
                    'from_published': str(self.stats[cve].from_published),
                    'language': self.stats[cve].language,
                    'foundHash' : self.stats[cve].foundHash,
                    'fixHash' : self.stats[cve].fixHash
                }
                cve_table.insert(record)

    # this is a temporary solution so the use of the function may not be supported in the future
    # as such it is not flexible
    def insert_mysql(self):
        host = "s1.sootsoft.hu"
        user = "cve_data"
        password = "aDjA4vaEsIjn5ZCF"
        con = pymysql.connect(
            host=host,
            user=user,
            password=password,
            cursorclass=pymysql.cursors.DictCursor
            )

        insertion_text = """INSERT INTO cve_data.test_data
        (cve_id, project, cwe_group, active_contributor_count,
         commit_count, deletions, insertions, total_lines_changed, files_changed,
          time_elapsed, found_date, fixed_date, severity, base_score,
           from_published, language)
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""

        for cve in self.stats:
            con.cursor().execute(
                insertion_text,
                (
                    cve,
                    self.stats[cve].project,
                    self.stats[cve].cwe,
                    self.stats[cve].active_contributor_count,
                    self.stats[cve].commit_count,
                    self.stats[cve].insertions,
                    self.stats[cve].deletions,
                    self.stats[cve].total_lines_changed,
                    self.stats[cve].files_changed,
                    str(self.stats[cve].time_elapsed),
                    str(self.stats[cve].found_date),
                    str(self.stats[cve].fixed_date),
                    self.stats[cve].severity,
                    self.stats[cve].base_score,
                    str(self.stats[cve].from_published),
                    self.stats[cve].language
                )
            )
        con.commit()
        con.close()

    # this is a temporary solution for uploading stats to the mysql db
    def calculate_stats_mysql(self):
        print('Updating stats')

        tm = TimeManager()

        host = "s1.sootsoft.hu"
        user = "cve_data"
        password = "aDjA4vaEsIjn5ZCF"
        con = pymysql.connect(
            host=host,
            user=user,
            password=password,
            cursorclass=pymysql.cursors.DictCursor
        )

        cursor = con.cursor()
        cursor.execute('SELECT DISTINCT language FROM cve_data.test_data')

        langs = cursor.fetchall()
        for lang in progressbar.progressbar(langs):
            stats = dict()
            cursor.execute('SELECT * FROM cve_data.test_data WHERE language = %s', lang['language'])
            for data in cursor.fetchall():
                cve = data['cve_id']
                stats[cve] = CveStatistics(
                    commit_count=data['commit_count'],
                    base_score=data['base_score'],
                    insertions=data['insertions'],
                    deletions=data['deletions'],
                    files_changed=data['files_changed'],
                    time_elapsed=tm.to_timedelta(data['time_elapsed']),
                    from_published=tm.to_timedelta(data['from_published'])
                )

            global_stats = GlobalStatistics(stats)
            correlations = global_stats.calculate_correlation()
            averages = global_stats.calculate_averages()
            deviations = global_stats.calculate_deviations()

            cursor.execute('SELECT language FROM cve_data.cve_stats WHERE language = %s', lang['language'])
            if cursor.fetchall():
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "correlation"', (str(correlations['corr']), lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "corr_files"', (str(correlations['corr_files']), lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "corr_lines"', (str(correlations['corr_lines']), lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "avg_fix"', (averages['avg_time'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "avg_fix_published"', (averages['avg_published'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "avg_files"', (averages['avg_files_changed'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "avg_lines"', (averages['avg_lines_changed'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "median_time"', (averages['median_time'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "median_published"', (averages['median_published'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "median_lines"', (averages['median_lines'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "median_files"', (averages['median_files'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "std_fix"', (deviations['sd_time'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "std_published"', (deviations['sd_published'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "std_lines"', (deviations['sd_lines'], lang['language']))
                cursor.execute('UPDATE cve_data.cve_stats SET value = %s WHERE language = %s AND stat = "std_files"', (deviations['sd_files'], lang['language']))
            else:
                insertion_text = 'INSERT INTO cve_data.cve_stats VALUES (%s, %s, %s)'
                cursor.execute(insertion_text, (lang['language'], 'correlation', str(correlations['corr'])))
                cursor.execute(insertion_text, (lang['language'], 'corr_files', str(correlations['corr_files'])))
                cursor.execute(insertion_text, (lang['language'], 'corr_lines', str(correlations['corr_lines'])))
                cursor.execute(insertion_text, (lang['language'], 'avg_fix', averages['avg_time']))
                cursor.execute(insertion_text, (lang['language'], 'avg_fix_published', averages['avg_published']))
                cursor.execute(insertion_text, (lang['language'], 'avg_files', averages['avg_files_changed']))
                cursor.execute(insertion_text, (lang['language'], 'avg_lines', averages['avg_lines_changed']))
                cursor.execute(insertion_text, (lang['language'], 'median_time', averages['median_time']))
                cursor.execute(insertion_text, (lang['language'], 'median_published', averages['median_published']))
                cursor.execute(insertion_text, (lang['language'], 'median_lines', averages['median_lines']))
                cursor.execute(insertion_text, (lang['language'], 'median_files', averages['median_files']))
                cursor.execute(insertion_text, (lang['language'], 'std_fix', deviations['sd_time']))
                cursor.execute(insertion_text, (lang['language'], 'std_published', deviations['sd_published']))
                cursor.execute(insertion_text, (lang['language'], 'std_lines', deviations['sd_lines']))
                cursor.execute(insertion_text, (lang['language'], 'std_files', deviations['sd_files']))
        con.commit()
        con.close()

    def calculate_stats(self):
        print('Updating statistics')
        stats_table = airtable.Airtable('appX3T3GA3Tim89PA', 'stats', api_key=self.apikey)
        table = airtable.Airtable('appX3T3GA3Tim89PA', 'cve_data', api_key=self.apikey)

        tm = TimeManager()
        stats = dict()
        cves = table.get_all(fields=['cve_id', 'commit_count', 'base_score', 'insertions', 'deletions', 'files_changed', 'time_elapsed', 'from_published', 'language'])
        for record in cves:
            try:
                record['fields']['base_score']
            except KeyError:
                continue

            lang = record['fields']['language']
            cve = record['fields']['cve_id']

            if not stats.get(lang):
                stats[lang] = dict()

            stats[lang][cve] = CveStatistics(
                commit_count=record['fields']['commit_count'],
                base_score=record['fields']['base_score'],
                insertions=record['fields']['insertions'],
                deletions=record['fields']['deletions'],
                files_changed=record['fields']['files_changed'],
                time_elapsed=tm.to_timedelta(record['fields']['time_elapsed']),
                from_published=tm.to_timedelta(record['fields']['from_published'])
            )

        for lang in stats:
            gs = GlobalStatistics(stats[lang])
            correlations = gs.calculate_correlation()
            averages = gs.calculate_averages()
            deviations = gs.calculate_deviations()

            records = dict()
            records['correlation'] = str(correlations['corr'])
            records['corr_files'] = str(correlations['corr_files'])
            records['corr_lines'] = str(correlations['corr_lines'])
            records['avg_fix'] = averages['avg_time']
            records['avg_fix_published'] = averages['avg_published']
            records['avg_files'] = averages['avg_files_changed']
            records['avg_lines'] = averages['avg_lines_changed']
            records['median_time'] = averages['median_time']
            records['median_published'] = averages['median_published']
            records['median_files'] = averages['median_files']
            records['median_lines'] = averages['median_lines']
            records['std_fix'] = deviations['sd_time']
            records['std_published'] = deviations['sd_published']
            records['std_files'] = deviations['sd_files']
            records['std_lines'] = deviations['sd_lines']

            if len(stats_table.search('language', lang)) == 0:
                for key in records.keys():
                    record = {
                        'language': lang,
                        'stat': key,
                        'value': records[key]
                    }
                    stats_table.insert(record)
            else:
                for key in records.keys():
                    stat_record = stats_table.match('stat_key', lang + key)
                    stats_table.update(stat_record['id'], {'value': records[key]})

class TimeManager(object):
    def __init__(self):
        pass

    def to_timedelta(self, date_string):
        days = ''
        timestamp = ''
        if date_string.find('days') != -1:
            days, timestamp = date_string.split(" days, ")
        elif date_string.find('day') != -1:
            days, timestamp = date_string.split(" day, ")
        else:
            timestamp = date_string

        t = datetime.datetime.strptime(timestamp, "%H:%M:%S")
        if days:
            return datetime.timedelta(days=int(days), hours=t.hour, minutes=t.minute, seconds=t.second)

        return datetime.timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)

# since gitlogparser is a separate tool, the reparsing of dates is needed
def dateParser(date_string):
    try:
        return iso8601.parse_date(date_string)
    except Exception as ex:
        print(ex)
        sliced_date = date_string[:date_string.rfind(' ')]
        try:
            return pytz.UTC.localize(datetime.datetime.strptime(sliced_date, '%a %b %d %H:%M:%S %Y'))
        except Exception as ex:
            print(ex)
            return None
# simple class used to simulate command line arguments
class DummyArgs(object):
        def __init__(self, url='', password='', apikey='', store_db='', language='Unkown', dir='', json='', ght=''):
            self.directory = dir
            self.online = url
            self.password = password
            self.apikey = apikey
            self.store_db = store_db
            self.language = language
            self.json=json
            self.github_token = ght
            self.no_merge = True
        
        def __str__(self):
            return f"""
                    online: {self.online},
                    dir: {self.directory},
                    pw: {self.password},
                    apikey: {self.apikey},
                    store_db: {self.store_db},
                    lang: {self.language},
                    json: {self.json},
                    ght: {self.github_token}
                    """